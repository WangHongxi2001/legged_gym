# 强化学习问题记录

## 样本量与mini batch size

　　在一个 demo 任务（电机转速闭环）中，num_steps_per_env 即每次 PPO 迭代经过的步数过小，进而没有足够多的样本量用于学习，导致算法无法收敛。随着每次迭代经过的步数增多，算法逐渐得以收敛，但当大到一定程度后学习过程又会出现无法收敛的情况。猜测可能是因为步数增多后每次梯度下降的样本量太大，故通过增大 num_mini_batches，即缩小 mini batch 的大小可最终使算法收敛。

## 多个跟踪 reward 情况下的学习

　　在一个 demo 任务（电机转速闭环、腿长跟踪、腿摆角跟踪）中，存在电机转速、腿长与角度三个跟踪目标，三者 reward 均为 tracking_sigma 函数且 reward 权重相等。在默认参数下学习，腿长与角度两个跟踪目标很快达成，但电机转速无法收敛，通过增大并行环境数目即增加样本量，并增大 num_mini_batches 可一定程度缓解腿长与角度跟踪目标收敛后电机转速难以收敛的问题。